# üìç Ubicaci√≥n de la Comparaci√≥n Sistema vs LLM

## Frontend

La comparaci√≥n est√° integrada en: **`Frontend/src/pages/Results.tsx`**

### L√≠neas 177-180:
```tsx
{/* Comparaci√≥n Sistema vs LLM */}
<Show when={result()?.validacion_complejidades}>
  <ComparisonTable validacion={result()!.validacion_complejidades!} />
</Show>
```

### Posici√≥n en la p√°gina:
1. Header (t√≠tulo + botones)
2. Resumen Ejecutivo
3. **Complejidades Computacionales** ‚Üê Aqu√≠ est√°n los badges O(n), Œò(n), Œ©(1)
4. **‚Üí COMPARACI√ìN SISTEMA VS LLM ‚Üê** ‚úÖ **AQU√ç**
5. Clasificaci√≥n ML
6. Pseudoc√≥digo Validado
7. Flowchart
8. Reporte Completo en Markdown
9. Errores (si existen)
10. Secci√≥n de descarga destacada

## Backend

### Generaci√≥n de la validaci√≥n:
**Archivo:** `Backend/flujo_analisis.py`
**L√≠neas:** ~362-380 (FASE 8.5)

```python
# ==================== FASE 8.5: VALIDACI√ìN CON LLM ====================
try:
    complejidades_para_validar = {
        'mejor_caso': complejidades['complejidades'].get('mejor_caso', 'N/A'),
        'caso_promedio': complejidades['complejidades'].get('caso_promedio', 'N/A'),
        'peor_caso': complejidades['complejidades'].get('peor_caso', 'N/A')
    }
    
    validacion_resultado = self.validador_complejidades.validar_complejidades(
        pseudocodigo=pseudocodigo,
        complejidades_sistema=complejidades_para_validar,
        algorithm_name=algorithm_name
    )
    
    resultado['validacion_complejidades'] = validacion_resultado
except Exception as e:
    logger.error(f"Error en validaci√≥n con LLM: {str(e)}")
```

### Endpoint:
**Archivo:** `Backend/core/analizador/router.py`
**Endpoint:** `POST /analisis/analizar-con-reporte`
**L√≠nea:** ~189

El campo `validacion_complejidades` se incluye autom√°ticamente en la respuesta porque est√° en el `resultado` del flujo.

## ‚ùó Problema: Reporte no se muestra

### Causa probable:
1. El componente est√° esperando `result()?.reporte_markdown`
2. El backend genera el reporte en la FASE 9
3. Puede haber un error en esa fase que impide que el reporte se genere

### Verificaci√≥n:
```bash
# Terminal 1: Iniciar servidor
cd Backend
python -m uvicorn app:app --reload --port 8000

# Terminal 2: Probar endpoint
cd Backend
python test_endpoint_reporte.py
```

### Revisar en consola del backend:
Buscar mensajes como:
- `[OK] Reporte guardado en: ...`
- `[WARN] Error generando reporte: ...`

### Soluci√≥n temporal:
Si el reporte no se genera, el resto de la p√°gina (incluida la comparaci√≥n) deber√≠a mostrarse de todas formas porque usa `Show when={result()?.reporte_markdown}` que solo muestra esa secci√≥n si existe.

## üéØ Pasos para Verificar

1. **Iniciar Backend:**
   ```bash
   cd Backend
   python -m uvicorn app:app --reload --port 8000
   ```

2. **Iniciar Frontend:**
   ```bash
   cd Frontend
   npm run dev
   ```

3. **Probar:**
   - Ir a http://localhost:5173/validador
   - Ingresar pseudoc√≥digo
   - Hacer clic en "Analizar"
   - Ver resultados

4. **Verificar comparaci√≥n:**
   - Debe aparecer una tarjeta "üîç Validaci√≥n con LLM: Comparaci√≥n Sistema vs IA"
   - Con tabla comparativa
   - Badge de confianza
   - Estado de concordancia

## üêõ Debug

### Si no aparece la comparaci√≥n:
1. Abrir DevTools (F12)
2. Ver Console para errores
3. Ver Network ‚Üí buscar la petici√≥n `/analisis/analizar-con-reporte`
4. Ver la respuesta JSON y confirmar que existe `validacion_complejidades`

### Si no aparece el reporte:
1. Ver logs del backend
2. Buscar la FASE 9: GENERACI√ìN DE REPORTE
3. Ver si hay alg√∫n error en `agenteReportador`

## ‚úÖ Estado Actual

- ‚úÖ Backend: Validaci√≥n implementada y funcional
- ‚úÖ Frontend: Componente creado e integrado
- ‚úÖ API: Endpoint configurado
- ‚ö†Ô∏è  Reporte: Necesita verificaci√≥n (puede haber error en generaci√≥n)
